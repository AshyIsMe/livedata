{"id":"livedata-0k0","title":"Log all println outputs to journald as well as stdout","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-25T17:34:02.450256519+10:00","created_by":"aaron","updated_at":"2026-01-25T17:34:02.450256519+10:00"}
{"id":"livedata-187","title":"Epic: Modernize DuckDB Table Schema","description":"Update the DuckDB table schema to use proper data types instead of storing everything as TEXT/JSON. This will improve query performance, enable better parquet compression, and provide type safety.\n\n## Current Issues\n- Timestamp stored as TEXT (RFC3339 strings) instead of proper DATETIME\n- Log fields stored as JSON making queries inefficient  \n- No proper data types for common fields like priority, PID, etc.\n\n## Proposed New Schema\n\n\n## Breaking Changes\n⚠️ This change breaks compatibility with existing data directories as requested.","status":"closed","priority":2,"issue_type":"epic","created_at":"2026-01-18T11:53:13.581728337+10:00","created_by":"aaron","updated_at":"2026-01-18T15:03:00.430665977+10:00","closed_at":"2026-01-18T15:03:00.430665977+10:00","close_reason":"Closed"}
{"id":"livedata-1j1","title":"Column sorting in web UI doesn't re-run query with new sort order","description":"The column sorting feature (livedata-vux) currently only sorts the limited results that were already retrieved, not the entire dataset. When a user clicks a column header to sort, it should re-run the database query with the new ORDER BY clause to sort the complete result set.\n\nCurrent Behavior: The sortable column headers were implemented and the api_search() function correctly uses the sort and sort_dir parameters in its SQL queries. However, the search_ui() function that serves the HTML interface has its own separate query building logic that was not updated to use these parameters.\n\nExpected Behavior: Clicking a column header should pass the sort and sort_dir parameters to the backend, re-execute the database query with ORDER BY \u003ccolumn\u003e \u003cdirection\u003e, and return the first page of results sorted by the entire dataset.\n\nTechnical Details: File src/web_server.rs - The api_search() function (lines 219-338) correctly implements sorting. The search_ui() function (around lines 381-499) needs to be updated with the same sorting logic.\n\nSolution: Update the SQL query building in search_ui() to include the same ORDER BY logic that was added to api_search(), validating the sort column and direction parameters and applying them to the database query.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-24T10:13:31.108027181+10:00","created_by":"aaron","updated_at":"2026-01-24T11:59:28.092257521+10:00","closed_at":"2026-01-24T11:59:28.092257521+10:00","close_reason":"Closed"}
{"id":"livedata-1t8","title":"Web search results don't display despite showing result count","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-20T21:52:18.29635913+10:00","created_by":"aaron","updated_at":"2026-01-20T21:52:43.315643312+10:00","closed_at":"2026-01-20T21:52:43.315643312+10:00","close_reason":"wontfix"}
{"id":"livedata-21p","title":"Update Parquet Export Logic","description":"Modify write_minute_to_parquet() COPY statement to include all new columns, ensure parquet schema matches DuckDB schema","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:39:00.405677705+10:00","created_by":"aaron","updated_at":"2026-01-19T23:34:33.479412728+10:00","closed_at":"2026-01-19T23:34:33.479412728+10:00","close_reason":"Completed in commit f112eff - Parquet COPY statement updated to export all fields"}
{"id":"livedata-2uw","title":"Parquet files not written each minute in normal listening mode after historical processing","description":"After the application finishes processing the historical journal data and transitions to normal listening mode (real-time monitoring), parquet files are not being written at the expected one-minute intervals. The parquet writer should flush data to disk periodically during live operation, but this appears to not be happening.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-19T20:25:51.758615714+10:00","created_by":"aaron","updated_at":"2026-01-19T20:30:58.982645748+10:00","closed_at":"2026-01-19T20:30:58.982645748+10:00","close_reason":"Fixed by removing premature flush_all_minutes() call before the main loop. The issue was that calling flush_all_minutes() after historical processing wrote incomplete minutes to parquet files. When new entries arrived for those same minutes during real-time operation, the parquet files already existed and were skipped, causing data loss."}
{"id":"livedata-4q7","title":"Update parquet export for new schema","description":"Update `src/parquet_writer.rs` to export the new table schema to parquet files.\n\n### Required Changes\n\n1. **Update COPY SQL**\nModify the COPY command in `write_minute_to_parquet()` to include all new columns:\n\n\n2. **Update Column Order**\nEnsure the export column order matches the new table structure for consistency.\n\n3. **Test Parquet Schema**\nVerify that exported parquet files have the correct column types:\n- timestamp: TIMESTAMP\n- minute_key: TIMESTAMP  \n- message: STRING\n- priority: INT32\n- pid, _uid, _gid: INT32 (nullable)\n- All string fields: STRING\n- extra_fields: JSON\n\n### Files to Modify\n- `src/parquet_writer.rs`\n\n### Dependencies\n- Depends on: livedata-qp3 (DuckDBBuffer schema update)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-18T11:53:22.733364642+10:00","created_by":"aaron","updated_at":"2026-01-18T15:03:00.964609577+10:00","closed_at":"2026-01-18T15:03:00.964609577+10:00","close_reason":"Closed","dependencies":[{"issue_id":"livedata-4q7","depends_on_id":"livedata-187","type":"blocks","created_at":"2026-01-18T11:53:22.734619084+10:00","created_by":"aaron"},{"issue_id":"livedata-4q7","depends_on_id":"livedata-qp3","type":"blocks","created_at":"2026-01-18T11:53:49.262121189+10:00","created_by":"aaron"}]}
{"id":"livedata-561","title":"Testing \u0026 Validation","description":"Update unit tests to cover all new fields, add integration tests for parquet round-trip with all fields, test query performance","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:39:03.259124197+10:00","created_by":"aaron","updated_at":"2026-01-19T20:14:54.104435084+10:00","closed_at":"2026-01-19T20:14:54.104435084+10:00","close_reason":"Tests now passing - fixed DuckDB column/value mismatch and removed unused imports"}
{"id":"livedata-5wm","title":"Replace parquet storage with on-disk DuckDB","status":"closed","priority":1,"issue_type":"feature","created_at":"2026-01-23T23:12:20.212646283+10:00","created_by":"aaron","updated_at":"2026-01-23T23:18:01.668731013+10:00","closed_at":"2026-01-23T23:18:01.668731013+10:00","close_reason":"Feature complete: storage changed from parquet files to on-disk DuckDB"}
{"id":"livedata-8u3","title":"Update Field Extraction Logic","description":"Expand add_entry() and get_entries_for_minute() methods to handle all systemd fields individually, minimize extra_fields JSON usage","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:38:59.055366814+10:00","created_by":"aaron","updated_at":"2026-01-19T20:17:16.859057521+10:00","closed_at":"2026-01-19T20:17:16.859057521+10:00","close_reason":"Field extraction logic complete - all 76 fields extracted from LogEntry"}
{"id":"livedata-9e1","title":"Add All Systemd Journal Fields to DuckDB Schema and Parquet Exports","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:38:53.12486707+10:00","created_by":"aaron","updated_at":"2026-01-19T23:34:33.921484524+10:00","closed_at":"2026-01-19T23:34:33.921484524+10:00","close_reason":"Completed in commit f112eff - All subtasks complete"}
{"id":"livedata-ale","title":"Make timestamp column first (leftmost) in datagrid","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-26T15:20:47.341347461+10:00","created_by":"aaron","updated_at":"2026-01-26T15:20:47.341347461+10:00"}
{"id":"livedata-dkh","title":"Add Missing LogEntry Methods","description":"Add getter methods in LogEntry for all new systemd fields, organize by category (user/trusted/kernel/etc.)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:39:01.859778269+10:00","created_by":"aaron","updated_at":"2026-01-19T23:34:33.639899033+10:00","closed_at":"2026-01-19T23:34:33.639899033+10:00","close_reason":"Completed in commit f112eff - LogEntry has 75+ getter methods for all systemd fields"}
{"id":"livedata-dmb","title":"Perspective datagrid showing empty with no columns/rows","status":"closed","priority":1,"issue_type":"bug","created_at":"2026-01-26T12:01:15.112913244+10:00","created_by":"aaron","updated_at":"2026-01-26T12:01:27.606066042+10:00","closed_at":"2026-01-26T12:01:27.606066042+10:00","close_reason":"Fixed by improving Perspective initialization error handling. Root cause: default search time range (-1h) didn't include any data. Added better error logging."}
{"id":"livedata-dul","title":"Schema Analysis \u0026 Design","description":"Review current field extraction logic and design new table schema with appropriate data types for all 59+ systemd journal fields","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:38:55.337180831+10:00","created_by":"aaron","updated_at":"2026-01-19T23:34:33.3468652+10:00","closed_at":"2026-01-19T23:34:33.3468652+10:00","close_reason":"Completed in commit f112eff - DuckDB schema expanded to include all 59+ systemd journal fields"}
{"id":"livedata-fw3","title":"Remove ParquetWriter and parquet flushing logic","description":"Remove parquet_writer.rs and all parquet writing code from app_controller since data persists in on-disk DuckDB.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-23T23:12:27.816353954+10:00","created_by":"aaron","updated_at":"2026-01-23T23:18:01.494740739+10:00","closed_at":"2026-01-23T23:18:01.494740739+10:00","close_reason":"Removed parquet_writer.rs and updated lib.rs","dependencies":[{"issue_id":"livedata-fw3","depends_on_id":"livedata-jv1","type":"blocks","created_at":"2026-01-23T23:12:49.301562665+10:00","created_by":"aaron"},{"issue_id":"livedata-fw3","depends_on_id":"livedata-t9l","type":"blocks","created_at":"2026-01-23T23:12:49.377311952+10:00","created_by":"aaron"}]}
{"id":"livedata-jv1","title":"Modify DuckDBBuffer to use on-disk database","description":"Change DuckDBBuffer from in-memory to on-disk database at data_dir/livedata.duckdb. Keep same schema.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-23T23:12:27.699610952+10:00","created_by":"aaron","updated_at":"2026-01-23T23:15:24.913182395+10:00","closed_at":"2026-01-23T23:15:24.913182395+10:00","close_reason":"DuckDBBuffer now uses on-disk database at data_dir/livedata.duckdb"}
{"id":"livedata-kgm","title":"Web search results don't display despite showing result count","description":"## Bug Summary\nWeb search results don't display in the table even when the pagination shows results were found (e.g., '16 results').\n\n## Bug Location\n- File: src/web_server.rs\n- Result rows construction: lines 567-597\n- HTML template format call: lines 977-992\n\n## Subtasks\n1. Investigate why result_rows is empty when results exist\n2. Fix the HTML template or result row construction\n3. Test search with various queries to verify fix","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-20T21:52:47.459431092+10:00","created_by":"aaron","updated_at":"2026-01-20T22:18:50.398120599+10:00","closed_at":"2026-01-20T22:18:50.398120599+10:00","close_reason":"Fixed priority column type mismatch - was reading INTEGER as String"}
{"id":"livedata-l7q","title":"Documentation Updates","description":"Update code comments explaining all supported fields, document schema changes","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:39:04.659068669+10:00","created_by":"aaron","updated_at":"2026-01-19T23:34:33.780812489+10:00","closed_at":"2026-01-19T23:34:33.780812489+10:00","close_reason":"Completed in commit f112eff - SQL comments document all field categories"}
{"id":"livedata-lgn","title":"CTRL-C graceful shutdown hangs after printing message","description":"When pressing CTRL-C, the application prints the graceful shutdown message but then appears to hang instead of exiting cleanly.","status":"closed","priority":2,"issue_type":"bug","created_at":"2026-01-23T23:57:29.505431142+10:00","created_by":"aaron","updated_at":"2026-01-24T00:07:07.108913153+10:00","closed_at":"2026-01-24T00:07:07.108913153+10:00","close_reason":"Closed"}
{"id":"livedata-mzo","title":"Validation and quality assurance","description":"Comprehensive validation and quality assurance for the schema changes.\n\n### Required Checks\n\n1. **Run Test Suite**\n- Run `cargo test` to ensure all tests pass\n- Run `cargo test --release` for release mode testing\n- Verify no test regressions\n\n2. **Code Quality**\n- Run `cargo clippy` and fix any new lint issues\n- Run `cargo fmt --check` to ensure consistent formatting\n- Fix any compiler warnings\n\n3. **Real Data Testing**\n- Test with real journald data to verify field extraction works correctly\n- Verify common systemd fields are properly extracted\n- Check that uncommon fields are preserved in extra_fields\n\n4. **Parquet Validation**\n- Verify parquet files have proper column types when opened externally\n- Test parquet compression is working with new schema\n- Verify parquet files can be read by external tools (duckdb, pandas, etc.)\n\n5. **Performance Testing**\n- Benchmark common queries on new schema vs old\n- Verify indexing improvements on dedicated columns\n- Test INSERT performance with new schema\n\n6. **Integration Testing**\n- Test the full data pipeline: journald → LogEntry → DuckDB → parquet\n- Verify all existing functionality still works with new schema\n- Test error handling for invalid data\n\n### Success Criteria\n- All tests pass\n- No clippy warnings\n- Real journald data processes correctly\n- Parquet files have correct schema\n- Performance is maintained or improved\n\n### Dependencies\n- Depends on: livedata-qp3 (DuckDBBuffer schema update)\n- Depends on: livedata-4q7 (parquet export update)  \n- Depends on: livedata-vx7 (test updates)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-18T11:53:43.937006131+10:00","created_by":"aaron","updated_at":"2026-01-18T15:03:01.522988826+10:00","closed_at":"2026-01-18T15:03:01.522988826+10:00","close_reason":"Closed","dependencies":[{"issue_id":"livedata-mzo","depends_on_id":"livedata-187","type":"blocks","created_at":"2026-01-18T11:53:43.938437084+10:00","created_by":"aaron"},{"issue_id":"livedata-mzo","depends_on_id":"livedata-qp3","type":"blocks","created_at":"2026-01-18T11:53:56.366694307+10:00","created_by":"aaron"},{"issue_id":"livedata-mzo","depends_on_id":"livedata-4q7","type":"blocks","created_at":"2026-01-18T11:53:59.115359276+10:00","created_by":"aaron"},{"issue_id":"livedata-mzo","depends_on_id":"livedata-vx7","type":"blocks","created_at":"2026-01-18T11:54:11.417420756+10:00","created_by":"aaron"}]}
{"id":"livedata-o5s","title":"Integrate Perspective datagrid for results table","description":"Replace the current results table on the frontend with Perspective datagrid (https://perspective-dev.github.io/). Perspective provides high-performance data grid and visualization capabilities with features like sorting, filtering, pivoting, and aggregations.","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-25T08:53:14.207923817+10:00","created_by":"aaron","updated_at":"2026-01-25T15:29:59.397545176+10:00","closed_at":"2026-01-25T15:29:59.397545176+10:00","close_reason":"Closed"}
{"id":"livedata-oud","title":"Add datasource for tracking running processes over time","description":"Implement a new datasource to track running processes over time, similar to top/htop monitoring.\n\n## Requirements\n\n### Data Collection\n- Collect process data every 5 seconds\n- Capture process information including:\n  - PID (process ID)\n  - User/owner\n  - Command line (full cmdline)\n  - Memory usage (RSS, VSZ)\n  - CPU usage (percentage)\n  - Network usage\n  - Other relevant metrics from top/htop\n\n### DuckDB Schema\n- Create a new table with proper columns for each metric\n- Do NOT use a single JSON field - each metric should have its own column\n- Include timestamp column for time-series analysis\n\n### Permission Handling\n- Gracefully handle non-root execution\n- Collect all information available to the current user\n- Skip/null fields that require elevated permissions\n- Do NOT error out if some data is unavailable due to permissions\n\n### Integration\n- Integrate as a new datasource in the livedata system\n- Follow existing datasource patterns\n- Ensure data is queryable through the web search interface","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-25T10:27:21.574242256+10:00","created_by":"aaron","updated_at":"2026-01-25T10:46:08.146532406+10:00"}
{"id":"livedata-qjf","title":"Update DuckDB Table Schema","description":"Modify CREATE TABLE statement in DuckDBBuffer::new() to include all systemd fields with appropriate SQL data types","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-19T11:38:56.766291127+10:00","created_by":"aaron","updated_at":"2026-01-19T20:17:16.728778287+10:00","closed_at":"2026-01-19T20:17:16.728778287+10:00","close_reason":"DuckDB schema already updated with all systemd journal fields - fixed VALUES placeholder mismatch"}
{"id":"livedata-qns","title":"Add timechart split view above datagrid using Perspective split layout","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-26T15:20:47.46213377+10:00","created_by":"aaron","updated_at":"2026-01-26T15:20:47.46213377+10:00"}
{"id":"livedata-qp3","title":"Update DuckDBBuffer schema and field extraction logic","description":"Update `src/duckdb_buffer.rs` to implement the new table schema and field extraction logic.\n\n### Required Changes\n\n1. **Update Table Schema**\nReplace current schema with new TIMESTAMP columns and dedicated field columns.\n\n2. **Update add_entry() Method**\n- Extract common fields from `entry.fields` HashMap into dedicated columns\n- Implement type conversions (priority/pid/uid/gid to INTEGER)\n- Move unrecognized fields to `extra_fields` JSON column\n- Handle nullable fields correctly\n\n3. **Update Indexes**\nAdd new indexes for priority, hostname, systemd_unit columns.\n\n4. **Update Query Methods**\n- Update all methods to handle new column structure\n- Ensure proper handling of NULL values for nullable columns\n\n### Field Mappings\n- MESSAGE → message (TEXT)\n- PRIORITY → priority (INTEGER, 0-7)\n- _SYSTEMD_UNIT → systemd_unit (TEXT)\n- _HOSTNAME → hostname (TEXT)\n- _PID → pid (INTEGER, nullable)\n- _EXE → exe (TEXT)\n- SYSLOG_IDENTIFIER → syslog_identifier (TEXT)\n- SYSLOG_FACILITY → syslog_facility (TEXT)\n- _UID → _uid (INTEGER, nullable)\n- _GID → _gid (INTEGER, nullable)\n- _COMM → _comm (TEXT)\n- All other fields → extra_fields (JSON)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-18T11:53:18.320796385+10:00","created_by":"aaron","updated_at":"2026-01-18T15:03:00.778021452+10:00","closed_at":"2026-01-18T15:03:00.778021452+10:00","close_reason":"Closed","dependencies":[{"issue_id":"livedata-qp3","depends_on_id":"livedata-187","type":"blocks","created_at":"2026-01-18T11:53:18.329171134+10:00","created_by":"aaron"}]}
{"id":"livedata-rnv","title":"Update web_server to query on-disk DuckDB directly","description":"Replace read_parquet() queries with direct table queries against the shared on-disk DuckDB database.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-23T23:12:27.959857492+10:00","created_by":"aaron","updated_at":"2026-01-23T23:16:44.459829023+10:00","closed_at":"2026-01-23T23:16:44.459829023+10:00","close_reason":"Web server now queries journal_logs table directly instead of read_parquet()","dependencies":[{"issue_id":"livedata-rnv","depends_on_id":"livedata-jv1","type":"blocks","created_at":"2026-01-23T23:12:49.326698757+10:00","created_by":"aaron"}]}
{"id":"livedata-t9l","title":"Update app_controller to use persistent DuckDB","description":"Remove periodic parquet flushing, keep data in on-disk DuckDB. Update shutdown logic.","status":"closed","priority":1,"issue_type":"task","created_at":"2026-01-23T23:12:28.074048942+10:00","created_by":"aaron","updated_at":"2026-01-23T23:15:30.242328879+10:00","closed_at":"2026-01-23T23:15:30.242328879+10:00","close_reason":"Removed parquet flushing, simplified to persistent DuckDB storage","dependencies":[{"issue_id":"livedata-t9l","depends_on_id":"livedata-jv1","type":"blocks","created_at":"2026-01-23T23:12:49.351604407+10:00","created_by":"aaron"}]}
{"id":"livedata-vux","title":"Add sortable columns to web search results","description":"The columns in the web search results table should be sortable by clicking on column headers (timestamp, hostname, unit, priority, etc).","status":"closed","priority":3,"issue_type":"feature","created_at":"2026-01-23T23:58:23.448165487+10:00","created_by":"aaron","updated_at":"2026-01-24T08:24:49.991464787+10:00","closed_at":"2026-01-24T08:24:49.991464787+10:00","close_reason":"Closed"}
{"id":"livedata-vx7","title":"Update tests for new schema","description":"Update and add tests to handle the new DuckDB table schema.\n\n### Required Changes\n\n1. **Fix Existing Tests in duckdb_buffer.rs**\n- Update tests that expect old schema (timestamp TEXT, fields JSON)\n- Fix `test_add_and_retrieve_entry()` to handle new column structure\n- Update `test_buffer_stats()` and `test_delete_minute()` for new schema\n- Update all INSERT/SELECT statements in tests\n\n2. **Fix Existing Tests in parquet_writer.rs**\n- Update `test_parquet_writer_creation()` if needed\n- Update any tests that validate parquet export structure\n- Ensure tests work with new column types\n\n3. **Add New Tests**\n- Test type conversions (string to INTEGER for priority, pid, etc.)\n- Test field extraction logic from HashMap to dedicated columns\n- Test nullable field handling (NULL values for missing fields)\n- Test extra_fields JSON preservation for uncommon fields\n- Test timestamp handling as TIMESTAMP instead of TEXT\n\n4. **Test Edge Cases**\n- Invalid integer values in fields (should become NULL)\n- Missing common fields (should be NULL)\n- Empty extra_fields (no uncommon fields present)\n- Very long text fields in string columns\n\n### Files to Modify\n- `src/duckdb_buffer.rs` (test section)\n- `src/parquet_writer.rs` (test section)\n\n### Dependencies\n- Depends on: livedata-qp3 (DuckDBBuffer schema update)\n- Depends on: livedata-4q7 (parquet export update)","status":"closed","priority":2,"issue_type":"task","created_at":"2026-01-18T11:53:35.032670153+10:00","created_by":"aaron","updated_at":"2026-01-18T15:03:01.271265408+10:00","closed_at":"2026-01-18T15:03:01.271265408+10:00","close_reason":"Closed","dependencies":[{"issue_id":"livedata-vx7","depends_on_id":"livedata-187","type":"blocks","created_at":"2026-01-18T11:53:35.034143796+10:00","created_by":"aaron"},{"issue_id":"livedata-vx7","depends_on_id":"livedata-qp3","type":"blocks","created_at":"2026-01-18T11:53:51.717652018+10:00","created_by":"aaron"},{"issue_id":"livedata-vx7","depends_on_id":"livedata-4q7","type":"blocks","created_at":"2026-01-18T11:53:54.118778247+10:00","created_by":"aaron"}]}
{"id":"livedata-zbq","title":"Expand results page size to 100k rows","status":"open","priority":2,"issue_type":"task","created_at":"2026-01-26T15:20:47.222769192+10:00","created_by":"aaron","updated_at":"2026-01-26T15:20:47.222769192+10:00"}
